{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Product Review Classification\n",
    "\n",
    "## Business Understanding\n",
    "Our company wants a tool that will automatically classify product reviews as _positive_ or _negative_ reviews, based on the features of the review.  This will help our Product team to perform more sophisticated analyses in the future to help ensure customer satisfaction.\n",
    "\n",
    "## Data Understanding\n",
    "We have a labeled collection of 20,000 product reviews, with an equal split of positive and negative reviews. The dataset contains the following features:\n",
    "\n",
    " - `ProductId` Unique identifier for the product\n",
    " - `UserId` Unqiue identifier for the user\n",
    " - `ProfileName` Profile name of the user\n",
    " - `HelpfulnessNumerator` Number of users who found the review helpful\n",
    " - `HelpfulnessDenominator` Number of users who indicated whether they found the review helpful or not\n",
    " - `Time` Timestamp for the review\n",
    " - `Summary` Brief summary of the review\n",
    " - `Text` Text of the review\n",
    " - `PositiveReview` 1 if this was labeled as a positive review, 0 if it was labeled as a negative review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>PositiveReview</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B002QWHJOU</td>\n",
       "      <td>A37565LZHTG1VH</td>\n",
       "      <td>C. Maltese</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1305331200</td>\n",
       "      <td>Awesome!</td>\n",
       "      <td>This is a great product. My 2 year old Golden ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B000ESLJ6C</td>\n",
       "      <td>AMUAWXDJHE4D2</td>\n",
       "      <td>angieseashore</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1320710400</td>\n",
       "      <td>Was there a recipe change?</td>\n",
       "      <td>I have been drinking Pero ever since I was a l...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B004IJJQK4</td>\n",
       "      <td>AMHHNAFJ9L958</td>\n",
       "      <td>A M</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1321747200</td>\n",
       "      <td>These taste so bland.</td>\n",
       "      <td>Look, each pack contains two servings of 120 c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ProductId          UserId    ProfileName  HelpfulnessNumerator  \\\n",
       "0  B002QWHJOU  A37565LZHTG1VH     C. Maltese                     1   \n",
       "1  B000ESLJ6C   AMUAWXDJHE4D2  angieseashore                     1   \n",
       "2  B004IJJQK4   AMHHNAFJ9L958            A M                     0   \n",
       "\n",
       "   HelpfulnessDenominator        Time                     Summary  \\\n",
       "0                       1  1305331200                    Awesome!   \n",
       "1                       1  1320710400  Was there a recipe change?   \n",
       "2                       1  1321747200       These taste so bland.   \n",
       "\n",
       "                                                Text  PositiveReview  \n",
       "0  This is a great product. My 2 year old Golden ...               1  \n",
       "1  I have been drinking Pero ever since I was a l...               0  \n",
       "2  Look, each pack contains two servings of 120 c...               0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"reviews.csv\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has already been cleaned, so there are no missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProductId                 0\n",
       "UserId                    0\n",
       "ProfileName               0\n",
       "HelpfulnessNumerator      0\n",
       "HelpfulnessDenominator    0\n",
       "Time                      0\n",
       "Summary                   0\n",
       "Text                      0\n",
       "PositiveReview            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`PositiveReview` is the target, and all other columns are features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\"PositiveReview\", axis=1)\n",
    "y = df[\"PositiveReview\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "First, split into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000, 8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, prepare for modeling. The following `Pipeline` prepares all data for modeling.  It one-hot encodes the `ProductId`, applies a tf-idf vectorizer to the `Summary` and `Text`, keeps the numeric columns as-is, and drops all other columns.\n",
    "\n",
    "The following code may take up to 1 minute to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000, 11275)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def drop_irrelevant_columns(X):\n",
    "    return X.drop([\"UserId\", \"ProfileName\"], axis=1)\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    (\"drop_columns\", FunctionTransformer(drop_irrelevant_columns, validate=False)),\n",
    "    (\"transform_text_columns\", ColumnTransformer(transformers=[\n",
    "        (\"ohe\", OneHotEncoder(categories=\"auto\", handle_unknown=\"ignore\", sparse=False), [\"ProductId\"]),\n",
    "        (\"summary-tf-idf\", TfidfVectorizer(max_features=1000), \"Summary\"),\n",
    "        (\"text-tf-idf\", TfidfVectorizer(max_features=1000), \"Text\")\n",
    "        ()\n",
    "    ], remainder=\"passthrough\"))\n",
    "])\n",
    "\n",
    "X_train_transformed = pipeline.fit_transform(X_train)\n",
    "X_test_transformed = pipeline.transform(X_test)\n",
    "\n",
    "X_train_transformed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "Fit a `RandomForestClassifier` with the best hyperparameters.  The following code may take up to 1 minute to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=30, min_samples_split=15, random_state=42)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(\n",
    "    random_state=42,\n",
    "    n_estimators=100,\n",
    "    max_depth=30,\n",
    "    min_samples_split=15,\n",
    "    min_samples_leaf=1\n",
    ")\n",
    "rfc.fit(X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "We are using _accuracy_ as our metric, which is the default metric in Scikit-Learn, so it is possible to just use the built-in `.score` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.9826666666666667\n",
      "Test accuracy: 0.913\n"
     ]
    }
   ],
   "source": [
    "print(\"Train accuracy:\", rfc.score(X_train_transformed, y_train))\n",
    "print(\"Test accuracy:\", rfc.score(X_test_transformed, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train confusion matrix:\n",
      "[[7312  177]\n",
      " [  83 7428]]\n",
      "Test confusion matrix:\n",
      "[[2293  218]\n",
      " [ 217 2272]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Train confusion matrix:\")\n",
    "print(confusion_matrix(y_train, rfc.predict(X_train_transformed)))\n",
    "print(\"Test confusion matrix:\")\n",
    "print(confusion_matrix(y_test, rfc.predict(X_test_transformed)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Interpretation\n",
    "\n",
    "The tuned Random Forest Classifier model appears to be somewhat overfit on the training data, but nevertheless achieves 91% accuracy on the test data.  Of the 9% of mislabeled comments, about half are false positives and half are false negatives.\n",
    "\n",
    "Because this is a balanced dataset, 91% accuracy is a substantial improvement over a 50% baseline.  This model is ready for production use for decision support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.describe of         ProductId          UserId                  ProfileName  \\\n",
       "5514   B008EG5ADY  A2D7X9N3IV3S7B                deeper waters   \n",
       "1266   B003ZNXCE0  A1F465XE6126IS                         Dior   \n",
       "5864   B0002NYNYO  A1R4FNCN368HJ8               No Ax To Grind   \n",
       "15865  B000EHP62G   ASD537RN0CAK8              luvtoshopamazon   \n",
       "12892  B0013AFTGG  A24CTL534C3UN4                      eshemom   \n",
       "...           ...             ...                          ...   \n",
       "11284  B00017LEX4  A3K5LESV135GFJ                           -E   \n",
       "11964  B0000D94OR  A2WZ1IPU382OAG     S. R. Clark \"S.R. Clark\"   \n",
       "5390   B000H1217M  A1WUE9RWYX8F1K  Pamela Hohlbein \"phohlbein\"   \n",
       "860    B0060HVNNA  A3II2GQFQU1E4N                         Alex   \n",
       "15795  B00008CQVA   AGDS7B4DRSSMF                  P. Phillips   \n",
       "\n",
       "       HelpfulnessNumerator  HelpfulnessDenominator        Time  \\\n",
       "5514                      0                       0  1338249600   \n",
       "1266                      6                       7  1325289600   \n",
       "5864                     25                      25  1258934400   \n",
       "15865                     0                       1  1310169600   \n",
       "12892                     1                       1  1281139200   \n",
       "...                     ...                     ...         ...   \n",
       "11284                     1                       1  1315785600   \n",
       "11964                     3                       5  1297900800   \n",
       "5390                      0                       0  1346371200   \n",
       "860                       1                       1  1334016000   \n",
       "15795                    11                      14  1259452800   \n",
       "\n",
       "                                      Summary  \\\n",
       "5514                          Crunchy Oatmeal   \n",
       "1266                   This is made in China!   \n",
       "5864                            The real deal   \n",
       "15865           Great for you, great tasting!   \n",
       "12892                          Breathalicious   \n",
       "...                                       ...   \n",
       "11284  Super Crunchy and hearty (for popcorn)   \n",
       "11964             NOT A LOW CARBOHYDRATE FOOD   \n",
       "5390                                 training   \n",
       "860                               The COFFEE!   \n",
       "15795                       Very disappointed   \n",
       "\n",
       "                                                    Text  \n",
       "5514   This remains close to the top of my list for f...  \n",
       "1266   Very disappointing. It initially seemed like a...  \n",
       "5864   After I had a pizza with truffles and porcini ...  \n",
       "15865  These bars are so good, they don't even taste ...  \n",
       "12892  My dogs LOVE these,  and they are so much easi...  \n",
       "...                                                  ...  \n",
       "11284  These pop a more crunchy and seemingly more de...  \n",
       "11964  I might have missed the nutrtional information...  \n",
       "5390   Works quite well for my 8 month old Yorkie, He...  \n",
       "860    Excellent coffee. Aroma, Flavor, Body are exce...  \n",
       "15795  I recently purchased 5 cases of Wellness cat f...  \n",
       "\n",
       "[15000 rows x 8 columns]>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.describe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Data Preparation\n",
    "A train-test split has already been performed.\n",
    "Additionally, there is already a pipeline in place that drops some columns and converts all text columns into a numeric format for modeling.\n",
    "**Your only additional data preparation task is feature scaling.**  Tree-based models like Random Forest Classifiers do not require scaling, but TensorFlow neural networks do.\n",
    "There are two main strategies you can take for this task:\n",
    "#### Scaling within the existing pipeline\n",
    "If you are comfortable with pipelines, this is the more polished/professional route.\n",
    "1. Make a new pipeline, with a `StandardScaler` as the final step.  You can nest the steps of the previous pipeline inside of this new pipeline\n",
    "2. Generate a new `X_train_transformed_scaled` by calling `.fit_transform` on the new pipeline\n",
    "3. Generate a new `X_test_transformed_scaled` by calling `.transform` on the new pipeline\n",
    "#### Scaling after the pipeline has finished\n",
    "This is a better strategy if you are not as comfortable with pipelines.\n",
    "1. Instantiate a `StandardScaler` object\n",
    "2. Generate a new `X_train_transformed_scaled` by calling `.fit_transform` on the scaler object, after you have called `.fit_transform` on the pipeline\n",
    "3. Generate a new `X_test_transformed_scaled` by calling `.transform` on the scaler object, after you have called `.transform` on the pipeline\n",
    "If you are getting stuck at this step, skip it.  The model will still be able to fit, although the performance will be worse.  Keep in mind whether or not you scaled the data in your final analysis.\n",
    "### 2) Modeling\n",
    "Build a neural network classifier.  Specifically, use the `keras` submodule of the `tensorflow` library to build a multi-layer perceptron model with the `Sequential` interface.\n",
    "See the [`tf.keras` documentation](https://www.tensorflow.org/guide/keras/overview) for an overview on the use of `Sequential` models. See the [Keras layers documentation](https://keras.io/layers/core/) for descriptions of the `Dense` layer options.  \n",
    "1. Instantiate a `Sequential` model\n",
    "2. Add an input `Dense` layer.  You'll need to specify a `input_shape` = (11275,) because this is the number of features of the transformed dataset.\n",
    "3. Add 2 `Dense` hidden layers.  They can have any number of units, but keep in mind that more units will require more processing power.  We recommend an initial `units` of 64 for processing power reasons.\n",
    "4. Add a final `Dense` output layer.  This layer must have exactly 1 unit because we are doing a binary prediction task.\n",
    "5. Compile the `Sequential` model\n",
    "6. Fit the `Sequential` model on the preprocessed training data (`X_train_transformed_scaled`) with a b`batch_size` of 50 and `epochs` of 5 for processing power reasons.\n",
    "### 3) Model Tuning + Feature Engineering\n",
    "If you are running out of time, skip this step.\n",
    "Tune the neural network model to improve performance.  This could include steps such as increasing the units, changing the activation functions, or adding regularization.\n",
    "We recommend using using a `validation_split` of 0.1 to understand model performance without utilizing the test holdout set.\n",
    "You can also return to the preprocessing phase, and add additional features to the model.\n",
    "### 4) Model Evaluation\n",
    "Choose a final `Sequential` model, add layers, and compile.  Fit the model on the preprocessed training data (`X_train_transformed_scaled`, `y_train`) and evaluate on the preprocessed testing data (`X_test_transformed_scaled`, `y_test`) using `accuracy_score`.\n",
    "### 5) Technical Communication\n",
    "Write a paragraph explaining whether Northwind Trading Company should switch to using your new neural network model, or continue to use the Random Forest Classifier.  Beyond a simple comparison of performance, try to take into consideration additional considerations such as:\n",
    " - Computational complexity/resource use\n",
    " - Anticipated performance on future datasets (how might the data change over time?)\n",
    " - Types of mistakes made by the two kinds of models\n",
    "You can make guesses or inferences about these considerations.\n",
    "**Include at least one visualization** comparing the two types of models.  Possible points of comparison could include ROC curves, colorized confusion matrices, or time needed to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000, 11275)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "300/300 [==============================] - 3s 12ms/step - loss: 55063.2305 - accuracy: 0.4991 - val_loss: 27525.1992 - val_accuracy: 0.4978\n",
      "Epoch 2/5\n",
      "300/300 [==============================] - 2s 7ms/step - loss: 4405.5068 - accuracy: 0.5022 - val_loss: 416.8366 - val_accuracy: 0.4978\n",
      "Epoch 3/5\n",
      "300/300 [==============================] - 2s 7ms/step - loss: 807.3218 - accuracy: 0.5029 - val_loss: 825.2225 - val_accuracy: 0.4978\n",
      "Epoch 4/5\n",
      "300/300 [==============================] - 2s 7ms/step - loss: 296.9929 - accuracy: 0.5006 - val_loss: 2.6408 - val_accuracy: 0.4978\n",
      "Epoch 5/5\n",
      "300/300 [==============================] - 2s 8ms/step - loss: 2.6403 - accuracy: 0.5007 - val_loss: 2.6377 - val_accuracy: 0.4978\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a28a38828>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from keras import regularizers\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(units=6, activation='relu', input_shape=(X_train_transformed.shape[1],), kernel_regularizer=regularizers.l1(0.01)))\n",
    "model.add(Dense(units=64, activation='relu',kernel_regularizer=regularizers.l1(0.01)))\n",
    "model.add(Dense(units=64, activation='relu', kernel_regularizer=regularizers.l1(0.01)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer=\"Adam\", loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train_transformed, y_train, validation_data=(X_test_transformed, y_test), epochs=5, batch_size=50)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Northwinds trading company would be better served using a random forest model whose accuracy, though the model was overfit, was 90% versus the neural network model which had an accuracy metric of 50%.  Depending on the resurces of the company the amount of data that could be run on each model in the future may become an issue with neural networks as the amount of calculations given the type of model could cause the model to run very slow though it won't necessarily affect its accuracy.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-ad9bbd14fb12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mplot_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-58-ad9bbd14fb12>\u001b[0m in \u001b[0;36mplot_results\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0max1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0max2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0max1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0max1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "def plot_results(model):\n",
    "    fig, (ax1,ax2) = plt.subplots(1,2)\n",
    "\n",
    "    ax1.plot(model.history.epoch, model.history.history['loss'], label='loss')\n",
    "    ax1.plot(model.history.epoch, model.history.history['val_loss'], label='val_loss')\n",
    "    \n",
    "    ax1.legend()\n",
    "    \n",
    "    ax2.plot(model.history.epoch, model.history.history['recall'], label='recall')\n",
    "    ax2.plot(model.history.epoch, model.history.history['val_recall'], label='val_recall')\n",
    "\n",
    "    ax2.legend()\n",
    "    \n",
    "    \n",
    "plot_results(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
